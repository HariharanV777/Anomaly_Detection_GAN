# -*- coding: utf-8 -*-
"""mini project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1r_miKrwQr9iRWouHAE_ll6oLoljSzs6w



import numpy as np
import IPython.display as display
from matplotlib import pyplot as plt
import io
import base64

ys = 200 + np.random.randn(100)
x = [x for x in range(len(ys))]

fig = plt.figure(figsize=(4, 3), facecolor='w')
plt.plot(x, ys, '-')
plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)
plt.title("Sample Visualization", fontsize=10)

data = io.BytesIO()
plt.savefig(data)
image = F"data:image/png;base64,{base64.b64encode(data.getvalue()).decode()}"
alt = "Sample Visualization"
display.display(display.Markdown(F"""![{alt}]({image})"""))
plt.close(fig)


pip install numpy pandas scikit-learn tensorflow matplotlib

import pandas as pd
import requests

# URL of the dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data"

# Download the dataset
response = requests.get(url)
response.raise_for_status()  # Check if the request was successful

# Save the dataset as a CSV file
with open('sonar_dataset.csv', 'wb') as file:
    file.write(response.content)

# Load the dataset into a pandas DataFrame
column_names = [f'Attribute_{i}' for i in range(1, 61)] + ['Class']
sonar_df = pd.read_csv('sonar_dataset.csv', header=None, names=column_names)

# Display the first few rows of the dataset
print(sonar_df.head())

import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense
import requests

# URL of the dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data"

# Download the dataset
response = requests.get(url)
response.raise_for_status()  # Check if the request was successful

# Save the dataset as a CSV file
with open('sonar_dataset.csv', 'wb') as file:
    file.write(response.content)

# Load the dataset into a pandas DataFrame
column_names = [f'Attribute_{i}' for i in range(1, 61)] + ['Class']
sonar_data = pd.read_csv('sonar_dataset.csv', header=None, names=column_names)

# Preprocessing
X_sonar = sonar_data.iloc[:, :-1].values  # Use all features except labels
vibration_data = np.random.normal(0, 1, (1000, 20))  # 1000 samples with 20 features
X = np.hstack((X_sonar, vibration_data[:X_sonar.shape[0], :]))  # Align samples
X = StandardScaler().fit_transform(X)  # Normalize data

# Split data for training and testing
X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)

# Define and Train Autoencoder
input_dim = X_train.shape[1]  # Now X_train is defined and accessible
encoding_dim = 20  # Compression size
input_layer = Input(shape=(input_dim,))
encoded = Dense(encoding_dim, activation='relu')(input_layer)
decoded = Dense(input_dim, activation='sigmoid')(encoded)

autoencoder = Model(input_layer, decoded)
autoencoder.compile(optimizer='adam', loss='mse')
autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)

from sklearn.cluster import KMeans

# Extract latent space representations (encoded data)
encoder = Model(input_layer, encoded)
X_train_encoded = encoder.predict(X_train)

# Apply K-means clustering on latent space
kmeans = KMeans(n_clusters=2, random_state=42).fit(X_train_encoded)

# Predict cluster labels on test data
X_test_encoded = encoder.predict(X_test)
clusters = kmeans.predict(X_test_encoded)

# Assume cluster 0 is 'normal', and others are 'anomalies'
anomalies = clusters != 0
print(f"Number of anomalies detected: {np.sum(anomalies)}")

from sklearn.cluster import KMeans

# Extract latent space representations (encoded data)
encoder = Model(input_layer, encoded)
X_train_encoded = encoder.predict(X_train)

# Apply K-means clustering on latent space
kmeans = KMeans(n_clusters=2, random_state=42).fit(X_train_encoded)

# Predict cluster labels on test data
X_test_encoded = encoder.predict(X_test)
clusters = kmeans.predict(X_test_encoded)

# Assume cluster 0 is 'normal', and others are 'anomalies'
anomalies = clusters != 0
print(f"Number of anomalies detected: {np.sum(anomalies)}")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dense

# Load Sonar dataset (UCI)
sonar_data = pd.read_csv('/content/sonar_dataset.csv', header=None)

# Prepare features and labels
X_sonar = sonar_data.iloc[:, :-1].values  # Use all features except labels
y_sonar = sonar_data.iloc[:, -1].map({'R': 0, 'M': 1}).values  # Map labels: Rock=0, Mine=1

# Simulate another dataset (e.g., sensor vibrations)
vibration_data = np.random.normal(0, 1, (X_sonar.shape[0], 20))  # Align with sonar samples

# Concatenate both datasets for fusion
X = np.hstack((X_sonar, vibration_data))  # Align samples
X = StandardScaler().fit_transform(X)  # Normalize data

# Split data for training and testing
X_train, X_test, y_train, y_true = train_test_split(X, y_sonar, test_size=0.2, random_state=42)

# Define Autoencoder
input_dim = X_train.shape[1]
encoding_dim = 20  # Compression size

input_layer = Input(shape=(input_dim,))
encoded = Dense(encoding_dim, activation='relu')(input_layer)
decoded = Dense(input_dim, activation='sigmoid')(encoded)

autoencoder = Model(input_layer, decoded)
autoencoder.compile(optimizer='adam', loss='mse')

# Train the model
autoencoder.fit(X_train, X_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)

# Step 1: Reconstruct the input
X_test_reconstructed = autoencoder.predict(X_test)

# Step 2: Calculate reconstruction error
reconstruction_error_test = np.mean(np.square(X_test - X_test_reconstructed), axis=1)

# Step 3: Define a threshold for anomalies
threshold = np.percentile(reconstruction_error_test, 95)  # 95th percentile

# Step 4: Predict anomalies based on the threshold
y_pred = reconstruction_error_test > threshold  # True for anomalies, False for normal
y_pred_labels = y_pred.astype(int)

# Step 5: Compute the confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred_labels)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot()
plt.title('Confusion Matrix for Anomaly Detection')
plt.show()

# Print metrics
accuracy = np.sum(y_true == y_pred_labels) / len(y_true)
precision = np.sum((y_true == 1) & (y_pred_labels == 1)) / np.sum(y_pred_labels == 1)
recall = np.sum((y_true == 1) & (y_pred_labels == 1)) / np.sum(y_true == 1)
f1_score = 2 * (precision * recall) / (precision + recall)

print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1_score:.2f}')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.svm import OneClassSVM

# Load Sonar dataset (UCI)
sonar_data = pd.read_csv('/content/sonar_dataset.csv', header=None)

# Prepare features and labels
X_sonar = sonar_data.iloc[:, :-1].values  # Use all features except labels
y_sonar = sonar_data.iloc[:, -1].map({'R': 0, 'M': 1}).values  # Map labels: Rock=0, Mine=1

# Simulate another dataset (e.g., sensor vibrations)
vibration_data = np.random.normal(0, 1, (X_sonar.shape[0], 20))  # Align with sonar samples

# Concatenate both datasets for fusion
X = np.hstack((X_sonar, vibration_data))  # Align samples
X = StandardScaler().fit_transform(X)  # Normalize data

# Split data for training and testing
X_train, X_test, y_train, y_true = train_test_split(X, y_sonar, test_size=0.2, random_state=42)

# Train SVM for anomaly detection
svm = OneClassSVM(kernel='rbf', gamma='auto', nu=0.1)  # Adjust nu for sensitivity
svm.fit(X_train)

# Predict anomalies in the test set
y_pred = svm.predict(X_test)

# Convert predictions to binary labels: -1 for anomaly, 1 for normal
y_pred_labels = np.where(y_pred == -1, 1, 0)

# Step 5: Compute the confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred_labels)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot()
plt.title('Confusion Matrix for Anomaly Detection using SVM')
plt.show()

# Print metrics
accuracy = np.sum(y_true == y_pred_labels) / len(y_true)
precision = np.sum((y_true == 1) & (y_pred_labels == 1)) / np.sum(y_pred_labels == 1)
recall = np.sum((y_true == 1) & (y_pred_labels == 1)) / np.sum(y_true == 1)
f1_score = 2 * (precision * recall) / (precision + recall)

print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1_score:.2f}')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Load Sonar dataset (UCI)
sonar_data = pd.read_csv('/content/sonar_dataset.csv', header=None)

# Prepare features and labels
X_sonar = sonar_data.iloc[:, :-1].values  # Use all features except labels
y_sonar = sonar_data.iloc[:, -1].map({'R': 0, 'M': 1}).values  # Map labels: Rock=0, Mine=1

# Simulate another dataset (e.g., sensor vibrations)
vibration_data = np.random.normal(0, 1, (X_sonar.shape[0], 20))  # Align with sonar samples

# Concatenate both datasets for fusion
X = np.hstack((X_sonar, vibration_data))  # Align samples
X = StandardScaler().fit_transform(X)  # Normalize data

# Split data for training and testing
X_train, X_test, y_train, y_true = train_test_split(X, y_sonar, test_size=0.2, random_state=42)

# Train Random Forest for anomaly detection
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)

# Predict anomalies in the test set
y_pred_labels = rf.predict(X_test)

# Step 5: Compute the confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred_labels)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot()
plt.title('Confusion Matrix for Anomaly Detection using Random Forest')
plt.show()

# Print metrics
accuracy = np.sum(y_true == y_pred_labels) / len(y_true)
precision = np.sum((y_true == 1) & (y_pred_labels == 1)) / np.sum(y_pred_labels == 1)
recall = np.sum((y_true == 1) & (y_pred_labels == 1)) / np.sum(y_true == 1)
f1_score = 2 * (precision * recall) / (precision + recall)

print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1_score:.2f}')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Load Sonar dataset (UCI)
sonar_data = pd.read_csv('/content/sonar_dataset.csv', header=None)

# Prepare features and labels
X_sonar = sonar_data.iloc[:, :-1].values  # Use all features except labels
y_sonar = sonar_data.iloc[:, -1].map({'R': 0, 'M': 1}).values  # Map labels: Rock=0, Mine=1

# Simulate another dataset (e.g., sensor vibrations)
vibration_data = np.random.normal(0, 1, (X_sonar.shape[0], 20))  # Align with sonar samples

# Concatenate both datasets for fusion
X = np.hstack((X_sonar, vibration_data))  # Align samples
X = StandardScaler().fit_transform(X)  # Normalize data

# Split data for training and testing
X_train, X_test, y_train, y_true = train_test_split(X, y_sonar, test_size=0.2, random_state=42)

# Train Decision Tree for anomaly detection
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train, y_train)

# Predict anomalies in the test set
y_pred_labels = dt.predict(X_test)

# Step 5: Compute the confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred_labels)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot()
plt.title('Confusion Matrix for Anomaly Detection using Decision Tree')
plt.show()

# Print metrics
accuracy = np.sum(y_true == y_pred_labels) / len(y_true)
precision = np.sum((y_true == 1) & (y_pred_labels == 1)) / np.sum(y_pred_labels == 1)
recall = np.sum((y_true == 1) & (y_pred_labels == 1)) / np.sum(y_true == 1)
f1_score = 2 * (precision * recall) / (precision + recall)

print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1_score:.2f}')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout

# Load Sonar dataset (UCI)
sonar_data = pd.read_csv('/content/sonar_dataset.csv', header=None)

# Prepare features and labels
X_sonar = sonar_data.iloc[:, :-1].values  # Use all features except labels
y_sonar = sonar_data.iloc[:, -1].map({'R': 0, 'M': 1}).values  # Map labels: Rock=0, Mine=1

# Simulate another dataset (e.g., sensor vibrations)
vibration_data = np.random.normal(0, 1, (X_sonar.shape[0], 20))  # Align with sonar samples

# Concatenate both datasets for fusion
X = np.hstack((X_sonar, vibration_data))  # Align samples
X = StandardScaler().fit_transform(X)  # Normalize data

# Reshape for LSTM [samples, time steps, features]
X = X.reshape(X.shape[0], 1, X.shape[1])  # Add time step dimension

# Split data for training and testing
X_train, X_test, y_train, y_true = train_test_split(X, y_sonar, test_size=0.2, random_state=42)

# Build LSTM model
model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(X_train.shape[1], X_train.shape[2])))
model.add(Dropout(0.2))  # Dropout for regularization
model.add(Dense(1, activation='sigmoid'))  # Sigmoid for binary classification

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.1, verbose=1)

# Predict anomalies in the test set
y_pred_probs = model.predict(X_test)
y_pred_labels = (y_pred_probs > 0.5).astype(int).flatten()  # Thresholding at 0.5

# Step 5: Compute the confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred_labels)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot()
plt.title('Confusion Matrix for Anomaly Detection using LSTM')
plt.show()

# Print metrics
accuracy = np.sum(y_true == y_pred_labels) / len(y_true)
precision = np.sum((y_true == 1) & (y_pred_labels == 1)) / np.sum(y_pred_labels == 1)
recall = np.sum((y_true == 1) & (y_pred_labels == 1)) / np.sum(y_true == 1)
f1_score = 2 * (precision * recall) / (precision + recall)

print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.cluster import KMeans
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Load Sonar dataset (UCI)
sonar_data = pd.read_csv('/content/sonar_dataset.csv', header=None)

# Prepare features and labels
X_sonar = sonar_data.iloc[:, :-1].values  # Use all features except labels
y_sonar = sonar_data.iloc[:, -1].map({'R': 0, 'M': 1}).values  # Map labels: Rock=0, Mine=1

# Simulate another dataset (e.g., sensor vibrations)
vibration_data = np.random.normal(0, 1, (X_sonar.shape[0], 20))  # Align with sonar samples

# Concatenate both datasets for fusion
X = np.hstack((X_sonar, vibration_data))  # Align samples
X = StandardScaler().fit_transform(X)  # Normalize data

# Split data for training and testing
X_train, X_test, y_train, y_true = train_test_split(X, y_sonar, test_size=0.2, random_state=42)

# Train K-Means for anomaly detection
kmeans = KMeans(n_clusters=2, random_state=42)
kmeans.fit(X_train)

# Predict clusters for the test set
y_pred_labels = kmeans.predict(X_test)

# Assuming cluster 1 is for anomalies
# Map clusters to labels based on majority vote
# This part may require adjustment based on your specific clustering
pred_mapped = np.where(y_pred_labels == 1, 1, 0)  # Assume cluster 1 is anomalies

# Step 5: Compute the confusion matrix
conf_matrix = confusion_matrix(y_true, pred_mapped)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot()
plt.title('Confusion Matrix for Anomaly Detection using K-Means')
plt.show()

# Print metrics
accuracy = np.sum(y_true == pred_mapped) / len(y_true)
precision = np.sum((y_true == 1) & (pred_mapped == 1)) / np.sum(pred_mapped == 1)
recall = np.sum((y_true == 1) & (pred_mapped == 1)) / np.sum(y_true == 1)
f1_score = 2 * (precision * recall) / (precision + recall)

print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1_score:.2f}')

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

# Load Sonar dataset (UCI)
sonar_data = pd.read_csv('/content/sonar_dataset.csv', header=None)

# Prepare features and labels
X_sonar = sonar_data.iloc[:, :-1].values  # Use all features except labels
y_sonar = sonar_data.iloc[:, -1].map({'R': 0, 'M': 1}).values  # Map labels: Rock=0, Mine=1

# Simulate another dataset (e.g., sensor vibrations)
vibration_data = np.random.normal(0, 1, (X_sonar.shape[0], 20))  # Align with sonar samples

# Concatenate both datasets for fusion
X = np.hstack((X_sonar, vibration_data))  # Align samples
X = StandardScaler().fit_transform(X)  # Normalize data

# Split data for training and testing
X_train, X_test, y_train, y_true = train_test_split(X, y_sonar, test_size=0.2, random_state=42)

# Train KNN for anomaly detection
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Predict anomalies in the test set
y_pred_labels = knn.predict(X_test)

# Step 5: Compute the confusion matrix
conf_matrix = confusion_matrix(y_true, y_pred_labels)

# Display confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix)
disp.plot()
plt.title('Confusion Matrix for Anomaly Detection using KNN')
plt.show()

# Print metrics
accuracy = np.sum(y_true == y_pred_labels) / len(y_true)
precision = np.sum((y_true == 1) & (y_pred_labels == 1)) / np.sum(y_pred_labels == 1)
recall = np.sum((y_true == 1) & (y_pred_labels == 1)) / np.sum(y_true == 1)
f1_score = 2 * (precision * recall) / (precision + recall)

print(f'Accuracy: {accuracy:.2f}')
print(f'Precision: {precision:.2f}')
print(f'Recall: {recall:.2f}')
print(f'F1 Score: {f1_score:.2f}')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import plot_model

# Load the Sonar dataset
url = "https://archive.ics.uci.edu/ml/machine-learning-databases/undocumented/connectionist-bench/sonar/sonar.all-data"
column_names = [f'Attribute_{i}' for i in range(1, 61)] + ['Class']
sonar_df = pd.read_csv(url, header=None, names=column_names)

# Prepare features and labels
X = sonar_df.drop(columns=['Class']).values
y = (sonar_df['Class'] == 'M').astype(int).values  # 1 for 'M', 0 for 'R'

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Define the neural network model
model = Sequential()
model.add(Dense(32, input_dim=60, activation='relu'))
model.add(Dense(16, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

# Compile the model
model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

# Train the model and store the training history
history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_split=0.2, verbose=1)

# Plot model accuracy and loss
plt.figure(figsize=(12, 5))

# Plot accuracy
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.title('Model Accuracy')
plt.legend()

# Plot loss
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.title('Model Loss')
plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Model names and their corresponding accuracies
models = ['Autoencoder', 'SVM', 'Random Forest', 'Decision Tree', 'LSTM', 'K-Means', 'KNN']
accuracies = [45, 50, 79, 62, 83, 38, 88]  # Replace these values with your actual accuracies

# Create a bar graph
plt.figure(figsize=(10, 6))
plt.barh(models, accuracies, color='skyblue')
plt.xlabel('Accuracy (%)')
plt.title('Model Accuracies Comparison')
plt.xlim(0, 100)  # Set x-axis limit from 0 to 100
plt.grid(axis='x', linestyle='--', alpha=0.7)

# Add accuracy values on the bars
for index, value in enumerate(accuracies):
    plt.text(value, index, f'{value}%', va='center', fontsize=10)

plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
# Load Sonar dataset
sonar_data = pd.read_csv('/content/sonar_dataset.csv', header=None)

# Prepare features
X_sonar = sonar_data.iloc[:, :-1].values  # All features except the last
def build_generator(input_dim):
    model = Sequential()
    model.add(Dense(64, activation='relu', input_dim=input_dim))
    model.add(Dense(X_sonar.shape[1], activation='tanh'))  # Output shape same as input features
    return model

def build_discriminator(input_dim):
    model = Sequential()
    model.add(Dense(64, activation='relu', input_dim=input_dim))
    model.add(Dense(1, activation='sigmoid'))  # Binary classification
    return model
generator = build_generator(10)  # Random noise dimension
discriminator = build_discriminator(X_sonar.shape[1])

discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])

# Create GAN
noise = Input(shape=(10,))
generated_data = generator(noise)
discriminator.trainable = False
validity = discriminator(generated_data)

gan = Model(noise, validity)
gan.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))
def train_gan(epochs, batch_size=128):
    for epoch in range(epochs):
        # Select random real samples
        idx = np.random.randint(0, X_sonar.shape[0], batch_size)
        real_data = X_sonar[idx]

        # Generate fake data
        noise = np.random.normal(0, 1, (batch_size, 10))
        fake_data = generator.predict(noise)

        # Labels for real and fake data
        real_labels = np.ones((batch_size, 1))
        fake_labels = np.zeros((batch_size, 1))

        # Train discriminator
        d_loss_real = discriminator.train_on_batch(real_data, real_labels)
        d_loss_fake = discriminator.train_on_batch(fake_data, fake_labels)
        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)

        # Train generator
        noise = np.random.normal(0, 1, (batch_size, 10))
        g_loss = gan.train_on_batch(noise, real_labels)

        # Print progress
        if epoch % 1000 == 0:
            print(f"{epoch} [D loss: {d_loss[0]:.4f}, acc.: {100 * d_loss[1]:.2f}%] [G loss: {g_loss:.4f}]")
            # Generate synthetic samples
noise = np.random.normal(0, 1, (1000, 10))  # Generate 1000 samples
synthetic_data = generator.predict(noise)

# Convert to DataFrame for better handling
synthetic_data_df = pd.DataFrame(synthetic_data, columns=sonar_data.columns[:-1])
# Visualizing the synthetic data
plt.figure(figsize=(10, 6))
plt.scatter(synthetic_data_df.iloc[:, 0], synthetic_data_df.iloc[:, 1], alpha=0.5)
plt.title('Synthetic Data Generated by GAN')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.grid()
plt.show()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

# Load Sonar dataset
sonar_data = pd.read_csv('/content/sonar_dataset.csv', header=None)

# Prepare features and labels
X = sonar_data.iloc[:, :-1].values  # Use all features except labels

# Normalize the dataset
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Determine optimal number of clusters using the elbow method
inertia = []
silhouette_scores = []

for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))

# Plotting the elbow method
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(range(2, 11), inertia, marker='o')
plt.title('Elbow Method')
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia')

# Plotting silhouette scores
plt.subplot(1, 2, 2)
plt.plot(range(2, 11), silhouette_scores, marker='o')
plt.title('Silhouette Scores')
plt.xlabel('Number of Clusters')
plt.ylabel('Silhouette Score')

plt.show()

# Applying K-means with the optimal number of clusters (assume k=4 from elbow method)
optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
y_kmeans = kmeans.fit_predict(X_scaled)

# Visualizing the clusters
plt.figure(figsize=(10, 6))
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=y_kmeans, cmap='viridis', s=50)
centers = kmeans.cluster_centers_
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.75, marker='X')
plt.title('K-means Clustering on Sonar Dataset')
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.show()

# Calculate metrics
inertia_value = kmeans.inertia_
silhouette_avg = silhouette_score(X_scaled, y_kmeans)

# Print the metrics
print(f'Inertia: {inertia_value:.2f}')
print(f'Silhouette Score: {silhouette_avg:.2f}')

# Cluster distribution
unique, counts = np.unique(y_kmeans, return_counts=True)
cluster_distribution = dict(zip(unique, counts))
print('Cluster Distribution:', cluster_distribution)
